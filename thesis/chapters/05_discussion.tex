\section{Discussion}
\label{cha:discussion}

The research topic proved to have great depth and there are limitations to the experiment we have performed.

The benchmarked cluster configurations were limited, clusters of bigger sizes should also be evaluated, such as cluster of size 24.
We were not able to do it due to time constraints and limited resources due to lack of Google Cloud Platform tokens, which have not been issued to the University recently.
This would additionally lead to more reliable scalability findings.
Additionally we could have evaluated clusters with different VM sizes for nodes to benchmark the vertical scalability of the systems.
One could also test how would the systems perform with different storage classes.

We could have also repeated the benchmark over a longer time-frame in order to minimize the variance of the Cloud performance
(i.e., running the benchmarks in different weeks or months on different times over a few months in order to enhance reproducibility).
Furthermore, we could have used the entire generated dataset o 1 Billion e-scooter events instead of a subset of first 100 Million events.
We chose to only use a subset of events due to time constraint.

Future research could also look into whether change in the hardware of each node would play a role in scalability
(i.e., whether improving the hardware of certain nodes, like coordinator in MobilityDBC, would influence the scalability pattern).
MobilityDBC has a dedicated coordinator node which does not serve data directly, while in CrateDB all nodes participate and act as workers by default (this behavior is configurable).\footnote{\url{https://cratedb.com/docs/crate/reference/en/latest/config/node.html}}
We have decided not to give an extra node to MobilityDBC in order to keep the benchmark fair.
In real life deployment one can decide to add an extra coordinator node with less resources, as the Citus docs showcase\footnote{\url{[CITATION]}}.
One could also setup multiple coordinator nodes and load balance between them to and benchmark such setup.

We could have also benchmarked a wider range of concurrent connections to the systems.
We tried increasing the number of concurrent clients to 512, but have discovered that CrateDB started to drop connections during querying on 3 node cluster.
Our load-generator implementation does not support reconnection in worker threads, so we have decided not to benchmark larger amount of concurrent clients as the results would have little value.
We could have implemented the reconnection mechanism but due to time constrains we were not able to.

Additionally we could have evaluated the performance of different query types, such as updates and deletes, but due to time constraint we were not able to add this functionality to the load-generator.
Furthermore, one could benchmark more realistic workload than synthetic micro-benchmark, such as a trace based ones or use a mixed workload such as Yahoo Cloud Serving Benchmark (YCSB) \cite{cooperBenchmarkingCloudServing2010}.

It is also worth evaluating, how the systems perform when replication is configured and enabled.
We did not benchmark it as the MobilityDBC does not support replication natively and additional extensions are required, and we aimed to benchmark the systems under optimal conditions.

We have not evaluated the consistency, as MobilityDBC supports ACID compliant transactions and CrateDB guarantees only eventual consistency.
Future research could check the latency of CrateDB reaching its consistent state.
One could also evaluate the performance and scalability of MobilityDBC with transactions and without them.

As we were new to both DDBMS we could have overlooked possible query optimizations.
An experienced user of those systems might be able to get more performance of the same cluster sizes and possibly narrow the gap between CrateDB and MobilityDBC querying performance.
Nonetheless, the goal of this thesis was to benchmark the scalability of both DDBMS and we have accomplished that goal.

It would also be worth to find out what are the possible storage optimization gains in MobilityDBC, as it allows to identify and discard redundant observations from the trajectory data
(i.e., allows to remove data-points which can be interpolated).\footnote{\url{https://mobilitydb.com/project.html}}
CrateDB also supports interpolation of time-series data, but at the time of writing it does not support interpolation of geospatial points.

Additionally, future research could evaluate overhead of hosting those DDMBS on top of Kubernetes cluster compared to running the software of individual nodes directly on VMs without containerization.

% TODO: if time allows
% There is a big speed difference in iops and thus database performance on PVCs for kubernetes in azure cloud.
% The disks provided with VMs are much faster otherwise the disk speed (IOPS) depends on the allocated storage

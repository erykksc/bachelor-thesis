\section{Approach}
\label{cha:benchmarkdesign}

Our benchmark is designed to investigate how CrateDB and \mobilitydbc compare in terms of scalability under spatio-temporal workloads typical for IoT use cases.
Spatio-temporal aspects are the primary target of our benchmark, since \mobilitydbc value proposition is exactly that.
Our goal is not to advocate for one system over the other but to investigate their respective strengths and limitations in processing spatio-temporal (ST) data at scale.
To evaluate scalability, we have designed a benchmark seen in Figure~\ref{fig:benchmark_design}.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[
			font=\small,
			node distance=0.5cm,
			box/.style={
					rectangle,
					draw,
					text width=2.5cm,
					minimum height=1cm,
					align=center
				},
			client/.style={
					box,
					fill=blue!10,
					draw=blue!70
				},
			coordinator/.style={
					box,
					fill=orange!10,
					draw=orange!70,
				},
			worker/.style={
					box,
					fill=purple!10,
					draw=purple!70
				},
			group/.style={
					rectangle,
					draw,
					dashed,
					inner sep=0.2cm
				},
			arrow/.style={
					->,
					>=latex
				}
		]
		% Client components
		\node[client] (db_client) {\textbf{DB Client}};
		\node[client, below=of db_client] (datagen) {Data Generator};
		% Client connections
		\draw[arrow] (datagen) -- (db_client);
		% Coordinator node
		\node[coordinator, right=5cm of db_client] (cn) {
			\textbf{Master Node}};
		% Worker nodes
		\node[worker, below=of cn] (w2) {
			\textbf{Worker Node \\ 2}};
		\node[worker, left=of w2] (w1) {
			\textbf{Worker Node \\ 1}};
		\node[worker, right=of w2] (w3) {
			\textbf{Worker Node \\ N}};
		% Connections of coordinator to workers
		\draw[arrow] (cn) -- (w1);
		\draw[arrow] (cn) -- (w2);
		\draw[arrow] (cn) -- (w3);
		% Client to SUT connections
		\draw[arrow] (db_client) to[bend left=13] node[above] {Concurrent Queries/Writes} (cn);
		\draw[arrow] (cn) to[bend left=5] node[below] {Results} (db_client);

		% Group boxes
		\begin{pgfonlayer}{background}
			% Benchmarking client
			\node[group, fit=(db_client) (datagen)] (client) {};
			\node[above] at (client.north) {Load generator};

			% System Under test
			\node[group, fit=(cn) (w1) (w2) (w3)] (sut) {};
			\node[above] at (sut.north) {System Under Test};
		\end{pgfonlayer}
	\end{tikzpicture}
	\caption{
		Load generator issues synthetic spatio-temporal inserts and queries to a SUT cluster consisting of one master and N worker nodes.
	}
	\label{fig:benchmark_design}
\end{figure}

% Designed for cloud
\subsection{Infrastructure}
We designed this benchmark to be deployed and run on the cloud for two reasons.
First, we lacked access to multiple local machines with identical hardware, making resource heterogeneity a concern.
Second, a cloud-based deployment improves repeatability, as others can reproduce our setup without dedicated hardware.
Moreover, cloud deployments of distributed databases reflect real-world use cases, making the benchmark more relevant.

% infrastructure as code to allow reproducibility
To improve reproducibility and repeatability, all parts of the benchmark, including cloud resource definitions and software configuration, are expressed in code.
Using infrastructure as code allows multiple benchmark runs with the exact same configuration eliminating the risk of manual misconfiguration.
Additionally, systems having access to the identical resources makes the benchmark and comparison fair.

\subsection{Load generator}
% * this benchmark uses synthetic workload load generator
We decided to split the benchmark into two major components, the DDBMS cluster and a load generator.
The load generator simulates concurrent clients on a DDBMS cluster.
It uses a  synthetic micro benchmark workload with fixed user pool.
Simulating multiple clients at the same time strives to mimic the IOT workload where there are many devices issuing requests at the same time.
The data for the inserts is generated before the load generator runs, and the queries are generated on runtime.

By using a micro-benchmark workload, we sacrifice the realism of the application to improve understandability, simplify the development, and interpretability of the results.
We chose synthetic workloads to isolate system behavior and control variability by issuing similar queries and inserts.
While this reduces workload realism, it improves reproducibility and allows clearer attribution of performance effects.

Load generator is deployed in the cloud as well, close to the SUT instead of running it locally and sending queries through the internet.
This approach allows us to minimize latency and potential disturbances while the benchmark runs i.e., packet loss due to network problems.

\subsection{DDBMS Clusters}

Each DDBMS is deployed on a cluster of machines with the same specifications that meet or exceed each system's minimum requirements.
This allows fair comparison of both systems as they have access to same resources.
The infrastructure of the cluster is deployed in the cloud using infrastructure as code and the DDBMS is deployed on the infrastructure using an automation tool.
Using an automation tool allows repeatable deployment of the DDBMS in the same way as infrastructure as code allows repeatable deployment of the resources.

\subsection{Workloads and metrics}
% * explain which metrics have been chosen and why:
%   - latency
%   - throughput
%   - success rate (to see whether the system answers all queries and doesn't drop them)
During the benchmark run, the load generator measures the following metrics for the analysis afterwards:
(1) Latency - measured in milliseconds between sending the request to DDBMS and receiving a response from SUT,
(2) Success rate - this allows us to avoid a situation where one SUT would perform better by rejecting multiple queries,
(3) Throughput, by logging how many queries have been sent and the total time it took to do it, we calculate the throughput.

Latency and throughput were selected as primary metrics due to their relevance in real-time and high-volume IoT scenarios.
A system is considered to scale effectively if throughput increases sub-linearly or better with cluster size, and latency remains within acceptable bounds under increasing load.\cite{hossfeldComparingScalabilityCommunication2023}
Furthermore, throughput is important for IoT deployments as they are commonly deployed in large quantities and send a numerous inserts i.e., e-scooters deployed in a city which report their location and status.

% * explain which benchmark modes? will be tested and why 
%   - inserts - very realistic for iot usage where a lot of devices share their state/location
Moreover, we have identified three scenarios for the SUT which we plan to benchmark:
(1) Data insertion, a scenario where the clients send data to insert to the DDBMS.
By this we want to benchmark how the SUT performs when multiple clients insert data i.e., e-scooters deployed in a city reporting their status and location.
Here we find the write throughput metric extremely relevant.
% * explain which benchmark modes? will be tested and why 
%   - simple queries - important for real time use cases, like users of an app (finding the closest e-scooter)
(2) Simple read queries, in this scenario we try to simulate queries done by the users of a system i.e., people using the e-scooter app to find the closest e-scooters.
In this scenario we benchmark the read throughput, as it is a relevant metric for the clients of the system.
% * explain which benchmark modes? will be teste and why 
%   - complex queries - important for data analysis, to find out patterns and optimizations (e.g.,used by the e-scooter sharing company)
(3) Complex read queries, here we try to simulate a scenario of a complex queries used for analysis and optimizations i.e., queries done by the e-scooter company to find patterns in the traffic to optimize the placement of the e-scooter stations throughout a city or to gain insight into their data.

\subsection{Scalability}
% NOTE: multiple sizes of the cluster will be used (2,3,4,5) to establish scalability pattern
To establish a scalability pattern we vary cluster sizes to evaluate the horizontal scalability.
Each cluster size is benchmarked using several user pool sizes, simulating different concurrency levels.
Benchmarking the DDBMS using different client counts allow us to check whether the performance of the queries is improved, such as the response time, or the amount of simultaneous connections handled with acceptable latency.
This may lead to important findings as some companies may value the performance of the queries for their data analysis more than concurrent connections handled.
On the other hand a different company may prioritize performance for numerous concurrent devices i.e., an e-scooter company having a large fleet of vehicles.

Through this benchmark design, we aim to uncover scalability patterns of two distributed databases under realistic IoT-inspired workloads.
The evaluation results will inform system design choices for practitioners building large-scale spatio-temporal data processing pipelines.

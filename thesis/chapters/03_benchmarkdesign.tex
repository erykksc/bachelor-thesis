\section{Approach: Benchmark Design}
\label{cha:benchmarkdesign}

We approach the design by determining the requirements first and then creating a design matching them.

\subsection{Requirements}

We have identified the following requirements for our benchmark:
\begin{enumerate}
	\item \textbf{spatio-temporal workload} \label{req:spatiotemporal-workload}
		- benchmark needs to investigate the scalability of the systems under spatio-temporal workload.
		Spatio-temporal workload is our primary target of this benchmark, since \mobilitydbc value proposition is exactly that.
	\item \textbf{relevant} \label{req:relevant}
		- both systems should be deployed in a way that resembled the potential production deployments in real world and metrics collected are informative for potential use-cases and allow measuring the effectiveness of scaling of the systems.
	\item \textbf{portable} \label{req:portable}
		- it should be possible to run the benchmark in different environments with little no no changes applied to it.
		This is an important requirement for us as Google has decided to stop supplying credits for Google Cloud Platform to our University.
		Because of the lack of credits it was uncertain in which environment the benchmark would be run.
		Additionally, we should be able to develop the benchmark locally on a single machine before the access to a cluster is granted.
	\item \textbf{fair} \label{req:fair}
		- both DDBMS should have access to the same resources and should be treated in the same way.
		Our goal is not to advocate for one system over the other but to investigate their respective strengths and limitations in processing ST data at scale.
	\item \textbf{reproducible and repeatable} \label{req:reproducible-repeatable}
		- the benchmark needs to have a design that allow others to replicate the results and should be defined in a deterministic way.
		In no part of the benchmark should randomness without seed value be used.
	\item \textbf{understandable} \label{req:understandable}
		- benchmark design, workload, metrics, and results must be understandable for a Computer Science Bachelor Student.
		With that said, we should avoid adding steps and workloads to the benchmark that will complicate the understandability without bringing much value.
		It is important to handle the trade-off between understandability and relevancy.
\end{enumerate}

\subsection{Design}

% Designed for cloud
\subsubsection{Infrastructure}
We decided that the benchmark runs in the cloud because of the two reasons.
First, we lacked access to multiple local machines with identical hardware, making resource heterogeneity a concern.
Second, a Cloud based deployment improves repeatability, as others can reproduce our setup without buying dedicated hardware, meeting part of our repeatability requirement \ref{req:reproducible-repeatable}.
[CITATION]
Moreover, cloud deployments of distributed databases reflect real-world use cases, making the benchmark more relevant, matching our relevancy requirement \ref{req:relevant}.

% infrastructure as code to allow reproducibility
To match the requirement \ref{req:reproducible-repeatable} of reproducibility and repeatability, all parts of the benchmark, including cloud resource definitions and software configuration, are expressed in code\footnote{\url{[GITHUB-REPO]}}.
Using infrastructure as code allows multiple benchmark runs with the exact same configuration eliminating the risk of manual misconfiguration, ensuring repeatability in requirement \ref{req:reproducible-repeatable}.
Additionally, systems having access to the identical resources make the benchmark and comparison of the systems fair, matching requirement \ref{req:fair}.

We decided to split the benchmark into two major components, the DDBMS and a load-generator.

\subsubsection{DDBMS (SUT)}

Each DDBMS is deployed on a cluster of machines with the same specifications that meet or exceed each system's minimum requirements.
This allows fair comparison of both systems as they have access to same resources meeting our fairness requirement \ref{req:fair}.
The infrastructure of the cluster is deployed in the cloud using infrastructure as code and the DDBMS is deployed on the infrastructure using an automation tool.
Using an automation tool allows repeatable deployment of the DDBMS in the same way as infrastructure as code allows repeatable deployment of the resources.

In order to ensure portability requirement \ref{req:portable} we have decided to use containerization technology to run DDBMS software on each node.
As we use containerization for both databases we comply with the fairness requirement \ref{req:fair}.

\subsubsection{Load-generator}
The load-generator uses a closed synthetic micro benchmark workload generation using fixed user pool
(i.e., single benchmark run performs single type of queries on DDBMS using constant amount of users/clients connected to it, using generated data, where new requests are sent only when the previous ones get responses).
% TODO: [POSSIBlE-FIGURE]
Simulating multiple clients at the same time strives to mimic the IoT workload where many devices issue requests at the same time (e.g., multiple public buses share their current location).
The data for the inserts is generated before the load-generator runs, and the queries are generated at runtime based on the generated data and deterministic pseudo-randomness ensuring repeatability from requirement \ref{req:reproducible-repeatable}.

We use a micro-benchmark workload sacrificing the realism of the application to improve understandability, simplify the development, and interpretability of the results. This way we handle the trade-off between understandability and relevancy from requirement \ref{req:understandable}.
We chose synthetic workloads to isolate system behavior and control variability by issuing similar queries and inserts.
This way we reduce the workload realism but improve reproducibility from requirement \ref{req:reproducible-repeatable} and allows clearer attribution of performance effects, thus improving understandability from requirement \ref{req:understandable}.

We deploy the load-generator in the Cloud as well, not on the same resources as SUT (in order not to impact SUT performance) meeting the fairness requirement \ref{req:fair}.
Additionally, we run the load-generator in the Cloud instead of locally in order to minimize latency and potential disturbances (i.e., packet loss due to network problems).
This choice allows us to meet the repeatability requirement \ref{req:reproducible-repeatable}, as the load-generator hardware is set using VM size.

During the benchmark run, we take measurements of the following metrics using the load-generator for later analysis:
\begin{enumerate}
	\item Latency - measured in milliseconds between sending the request to DDBMS and receiving a response from SUT
	\item Throughput - how many requests have been successfully completed over time, measured in requests per second.
		We measure it by logging how many queries have been sent and received a successful response in time-windows.
	\item Success rate - measured in ratio between requests that received a successful response and total requests sent.
		This allows us to avoid a situation where one SUT would perform better by rejecting multiple queries and thus appearing more performant.
\end{enumerate}

Latency and throughput were selected as primary metrics due to their relevance in real-time and high-volume IoT scenarios.
A system is considered to scale effectively if throughput increases sub-linearly or better with cluster size, and latency remains within acceptable bounds under increasing load.\cite{hossfeldComparingScalabilityCommunication2023}
Furthermore, throughput is important for IoT deployments as they are commonly deployed in large quantities and send a numerous inserts i.e., e-scooters deployed in a city which report their location and status.

Moreover, we have identified three scenarios for the SUT which we plan to benchmark:
\begin{enumerate}
	\item \textbf{Data insertion} - a scenario where the clients send data to insert to the DDBMS.
By this we want to benchmark how the SUT performs when multiple clients insert data i.e., e-scooters deployed in a city reporting their status and location.
Here we find the write throughput metric extremely relevant.
	\item \textbf{Simple read queries} - in this scenario we try to simulate queries done by the users of a system i.e., people using the e-scooter app to find the closest e-scooters.
		In this scenario we benchmark the read throughput, as it is a relevant metric for the clients of the system.
	\item \textbf{Complex read queries} - here we try to simulate a scenario of a complex queries used for analysis and optimizations i.e., queries done by the e-scooter company to find patterns in the traffic to optimize the placement of the e-scooter stations throughout a city or to gain insight into their data.
\end{enumerate}

\subsubsection{Scalability}
To establish a scalability pattern we vary cluster sizes to evaluate the horizontal scalability.
Each cluster size is benchmarked using several user pool sizes, simulating different concurrency levels.
Benchmarking the DDBMS using different client counts allow us to check whether the performance of the queries is improved, such as the latency, or the throughput.
This may lead to important findings as some companies may value the performance of the queries for their data analysis more than concurrent connections handled.
On the other hand a different company may prioritize performance for numerous concurrent devices i.e., an e-scooter company having a large fleet of vehicles.

The resulting benchmark design can be seen in \cref{fig:benchmark-design}.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[
			font=\small,
			node distance=0.5cm,
			box/.style={
					rectangle,
					draw,
					text width=2.5cm,
					minimum height=1cm,
					align=center
				},
			client/.style={
					box,
					fill=blue!10,
					draw=blue!70
				},
			coordinator/.style={
					box,
					fill=orange!10,
					draw=orange!70,
				},
			worker/.style={
					box,
					fill=purple!10,
					draw=purple!70
				},
			group/.style={
					rectangle,
					draw,
					dashed,
					inner sep=0.2cm
				},
			arrow/.style={
					->,
					>=latex
				}
		]
		% Client components
		\node[client] (db_client) {\textbf{DB Client}};
		\node[client, below=of db_client] (datagen) {Data Generator};
		% Client connections
		\draw[arrow] (datagen) -- (db_client);
		% Coordinator node
		\node[coordinator, right=5cm of db_client] (cn) {
			\textbf{Master Node}};
		% Worker nodes
		\node[worker, below=of cn] (w2) {
			\textbf{Worker Node \\ 2}};
		\node[worker, left=of w2] (w1) {
			\textbf{Worker Node \\ 1}};
		\node[worker, right=of w2] (w3) {
			\textbf{Worker Node \\ N}};
		% Connections of coordinator to workers
		\draw[arrow] (cn) -- (w1);
		\draw[arrow] (cn) -- (w2);
		\draw[arrow] (cn) -- (w3);
		% Client to SUT connections
		\draw[arrow] (db_client) to[bend left=13] node[above] {Concurrent Queries/Writes} (cn);
		\draw[arrow] (cn) to[bend left=5] node[below] {Results} (db_client);
		% Group boxes
		\begin{pgfonlayer}{background}
			% Benchmarking client
			\node[group, fit=(db_client) (datagen)] (client) {};
			\node[above] at (client.north) {\textbf{Load-generator}};
			% System Under test
			\node[group, fit=(cn) (w1) (w2) (w3)] (sut) {};
			\node[above] at (sut.north) {\textbf{System Under Test}};
		\end{pgfonlayer}
	\end{tikzpicture}
	\caption{
		Load-generator issues synthetic spatio-temporal inserts and queries to a SUT cluster consisting of one master and N worker nodes.
	}
	\label{fig:benchmark-design}
\end{figure}

Through this benchmark design, we try to find scalability patterns of two DDBMS under realistic IoT-inspired workloads.
In the next section we will evaluate the design by implementing and applying it.

\section{Benchmark Design}
\label{cha:benchmarkdesign}

% \subsection{Kubernetes explanation}
%
% Important aspect of this benchmark was the portability as there was uncertainty where would the experiment be conducted.
% Three environments had to be taken into account.
% First, the cloud, initially it was planned to run the experiment on Google Cloud Platform but unfortunately, Google suddenly stopped providing credits for the University.
% Two, the server cluster of the University, with support for Kubernetes.
% Our machines, for testing and in case it wouldn't be possible to use different computers.
%
% Ultimately we have decided to go with Kubernetes, as it provided a way to run the database systems on all of the three environments, as well as increasing the benchmark relevance, as we assume the production systems are likely gonna be deployed in such environment.
% Additionally use of Kubernetes increased ease of use, as it is a common solution for distributed systems, along with robustness, as Kubernetes automatically revives dead nodes, allows to easily change replica count and extend the configuration for real life systems.
%
% For the data used for the benchmark we decided to use a synthetic data generator that would be run as a pod on the same Kubernetes cluster in order to minimize possible, latency and unknown variables on the network.


% Things to explain:
% * this benchmark is designed for cloud
\subsection{Infrastructure and Portability}
We design this benchmark to be deployed and run on the cloud for two reasons.
Firstly, we did not have access to multiple local machines with the same specification to ensure no bottlenecks.
Secondly, the repeatability and relevance of such approach is improved, as the benchmark can be replicated by other parties without them having access to a collection of devices and cloud deployed systems are a common (CITATION).

% * this benchmark uses infrastructure as code to allow reproducibility
% * this benchmark each ddbms is run on a Kubernetes cluster
To improve reproducibility and repeatability all parts of the benchmark are documented in code, this includes the infrastracture on the cloud provider.
Using infrastructure as code allows multiple runs with the exact same configuration without option of skipping checking in some setting on a cloud provider web admin console.
Documenting the DDBMS configuration through Kubernetes deployment files ensures that the software will be deployed with the same settings and versions using versioned container images.
Additionally by using Kubernetes, the benchmark can also be more easily developed on a local systems by using contenerization.
The local development has been an important consideration as the recent withdraw of google from issuing cloud tokens to our university, lead to uncertainty to where the benchmark will be deployed.
Kubernetes has been chosen over using bash scripts for installing DDBMS on multiple virtual machines for three reasons.
(1) It allows easier deployment of a DDBMS cluster, which is beneficial when deploying multiple cluster sizes and resetting their state by destroying them.
(2) The opportunity to experiment with a new technology also played a role, as we were familiar with working with Kubernetes before.
(3) Relevancy, as deployment of such clustered DDBMS systems is often done this way in production environments i.e., CrateDB offers a guide of deploying a production grade system, and offers ready Azure cloud deployment code for terraform (CRATEDB docs as an example)

% * explain that both ddbms will be run on the same resources (same vm sizes)
Using cloud and infrastructure as code allows us to deploy both CrateDB and MobilityDB on the exact same resources with the same configuration of them i.e., same virtual machine types/sizes.
This allows us to be fair towards both systems and provide them with the same resources.

\subsection{Load generator}
% * this benchmark uses synthetic workoad load generator
We decided to split the benchmark into two major components, the DDBMS cluster and a load generator.
Load generator will be a software simulating multiple clients on a DDBMS cluster by using multiple threads.
It will use a closed synthetic workload generation with fixed user pool with requests with random parameters generated on the fly based on query templates.
By using a synthetic workload we sacrifice the realism of the application by improving understandability and simplifying the development of the benchmark.
This approach allows easy data collection, compared to using multiple load generators running on multiple hosts and makes the benchmark cheaper to run.
We believe this strategy is viable, as the load generator primarily issues requests, which are i/o heavy, so the performance of the computer running the software shouldn't be a bottleneck.
Simulating multiple clients at the same time strives to mimic the IOT workload where there are many devices issuing requests at the same time.
% * this benchmark load generator is placed on the same VPC in the cloud as the cluster with running DDBMS
The load generator is also put on the cloud in the same virtual private cloud (VPC) as the DDBMS cluster.
Putting the load generator next to the SUT instead of running it on our local computer allows us to minimize latency and potential disturbances while the benchmark runs i.e., packages dropped due to network problems.


\subsection{Workloads and metrics}
% * explain which metrics have been chosen and why:
%   - latency
%   - throughput
%   - success rate (to see whether the system answers all queries and doesn't drop them)
During the benchmark run, the load generator will log the following metrics for the analysis afterwards:
(1) Latency in milliseconds between sending the request to DDBMS and receiving a response.
(2) Whether the request/query has been resolved successfully or not.
This allows us to avoid a situation where one SUT would perform better by rejecting multiple queries.
(3) Throughput, by logging how many queries have been sent and the total time it took to do it, we will calculate the throughput.
We chose this metric as it is an important in IOT devices as they are commonly deployed in great amounts and send a lot of data i.e., shareable e-scooters in a city which report their location and status.


% * explain which benchmark modes? will be tested and why 
%   - inserts - very realistic for iot usage where a lot of devices share their state/location
We have identified three scenarios for the SUT which we will test.
(1) Data insertion, a scenario where the clients will be sending data to insert to the DDBMS.
By this we want to benchmark how the SUT will perform when multiple clients try insert data i.e., e-scooters deployed in a city reporting their status and location.
Here we find the write throughput metric extremely relevant. (CITY SOME SOURCE)
% * explain which benchmark modes? will be tested and why 
%   - simple queries - important for real time usecases, like users of an app (finding the closest e-scooter)
(2) Simple read queries, in this scenario we try to simulate queries done by the users of a system i.e., people using the e-scooter app to find the closest e-scooters.
In this scenario we benchmark the read throughput, as it is a relevant metric for the clients of the system.
% * explain which benchmark modes? will be tested and why 
%   - complex queries - important for data analysis, to find out patterns and optimizations (e.g.,used by the e-scooter sharing company)
(3) Complex read queries, here we try to simulate a scenario of a complex queries used for analysis and optimizations i.e., queries done by the e-scooter company to find patterns in the traffic to optimize the placement of the e-scooter stations throughout a city or to gain insight into their data.

\subsection{Scalability}
% * mention and explain that multiple sizes of the cluster will be used (2,3,4,5) to establish scalability pattern
To establish a scalability pattern multiple sizes of a cluster need to be run.
We decided to benchmark the following sizes for the SUT clusters, 2, 3, 4, and 5.
Preferably we would benchmark the DDBMS on larger cluster sizes to improve relevance of our findings, but due to resource constrain we choose to do it on a set of small ones to reduce costs.
Despite the small difference in size between the sizes, we hope to see a pattern which would allow us to create conclusions about scalability.

% * different client amounts, different number of simultaneous connections simulated by the load generator (100, 1000, 10000)
To check how well the databases handle multiple simultaneous connections and try to find a pattern, we have decided to benchmark the SUT on following number of simultaneous clients 100, 1000, 10000.
Multiple configurations of amount of clients allow us to check whether the performance of the queries is improved, such as the response time, or the amount of simultaneous connections handled.
This could lead to important findings as some companies may value the performance of the queries for their data analysis more important compared to the simultaneous connections handled.
The latter one may be more important for a company where vast amount of IoT devices connect to the DDBMS at the same time.

% * include graph with the benchmark setup/infrastructure

% NOTE: doesn't add much more information, just restates what has been established
In this thesis we compare two DDBMSs, CrateDB and \mobilitydbc to assess their scalability in spatial-temporal workloads typical for IoT usecases.
Our goal is not to advocate for one system over the other but to investigate their respective strengths and limitation in processing ST data at scale.

\subsection{Design Objectives}
The benchmark should be relevant to real-world use cases, especially spatial-temporal workloads.
We accomplish that by modelling queries after common use cases like, time slice queries, window queries, and spatiotemporal joins.
We deploy the SUTs on a Kubernetes cluster in Microsoft Azure, resembling common deployment method of companies.
Such deployment using infrastructure as code, allows us to share the configuration of SUTs, as well as information of provisioned cloud resources, allowing reproducibility of our results.

The decision to choose Kubernetes over shell scripts or other automation tools such as Ansible has reduced understandability (as not everybody is familiar with container orchestration tools such as Kubernetes), but allowed us to improve portability, relevance and simplified development.
This trade off has been made as unexpectedly because of the cloud infrastructure provider, where we wanted to run the benchmark on initially, cut off credits for the university right before the start of the thesis writing.
Such change, made us switch to a solution that was more portable.
Using containerization, allowed us to develop on single host machine without creating VMs and also made it possible to run our benchmark on other cloud providers or university servers/Kubernetes cluster.
Furthermore, use of containers improves repeatability as the node will have exact same versions of the software installed on them.

The SUTs are benchmarked using equivalent conditions ensuring fairness by
(1) deploying them on the same cloud resources (processor types, memory, storage, and networking),
(2) running semantically analogous queries,
(3) connecting same amount of clients to them, and by
(3) inserting same amount of data.
Notably, \mobilitydbc~supports more complex queries than CrateDB and because of that we will benchmark queries supported by both systems.
We suspect that \mobilitydbc~ being full ACID compliant will result in reduced raw performance, but in this thesis we focus on scalability patterns so are making this trade-off to fairness.

Our benchmark evaluates scalability by measuring how each SUT performs under growing cluster sizes and data volumes.

\subsection{Quality Metrics}
To measure scalability we plot the Metric curves over increasing node counts.
We choose the following metrics for our benchmark
(1) Latency, it will be measured in milliseconds for both inserts and queries. We will create percentile breakdown (P50,P95, P99) to provide insight into tail latencies.
(2) Throughput, we will measure the number of records written or queries completed per second.

In the context of IoT devices, write throughput is important for the devices themselves, and the read latency is important for the users.
Because of this, the benchmark results should be relevant.

\subsection{Workload design}
We adopt a synthetic workload combined with a micro-benchmark approach where each run isolates one quality of system behavior (e.g., write latency, read throughput).
This avoids confounding metrics and ensures interpretable results. We have developed a custom synthetic workload generator, written in Golang, which will issue concurrent requests using lightweight goroutines to simulate a fixed large number of clients.
Golang has been chosen due to goroutines, which are lightweight threads (~2KB stack vs ~1MB stack when using OS threads) managed by the Go runtime, allowing us to simulate a large number of concurrent clients on a relatively small system.

For the write workload we will simulate IoT devices emitting time-stamped spatial data of %TODO: what will the generated data be of?
We will test batch and single-record inserts at varying ingestion rates to measure write throughput and latency.

Whereas for the read workload, we will simulate %TODO: what will we simulate exactly?
This will be performed by running following queries
(1) Temporal range queries (e.g., last hour of data per device).
(2) Spatial bounding box queries (e.g., devices within a bounding box).
(3) Spatio-temporal window queries (e.g., average speed over 10-minute intervals).

\subsection{Measurement}
We combined the load generator with the testing client in order for the same goroutine to issues requests as well as to measure their metrics.
This way we keep the measuring simple and understandable and don't overcomplicate the setup.

In order to minimize unknown variables, such as network latency, reliability, and interference of other devices, we put the load generator on the same Kubernetes cluster.
The load generator pod runs on a separate Kubernetes node, a different host machine, isolated from database nodes to prevent influencing SUT performance.
During the evaluation we monitor the resource usage of every node, focusing on the load generator in order to prevent it from becoming a bottleneck in the benchmark.
